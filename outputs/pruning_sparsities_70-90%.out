



----------------------------------------------------
----------------------------------------------------
Pruning started for sparsity ratio 0.8!
----------------------------------------------------
----------------------------------------------------
{'train_runtime': 19373.9251, 'train_samples_per_second': 0.013, 'train_steps_per_second': 0.013, 'train_loss': 2.658195972442627, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     2.6582
  train_runtime            = 5:22:53.92
  train_samples_per_second =      0.013
  train_steps_per_second   =      0.013
----------------------------------------------------
----------------------------------------------------
Pruning finished for sparsity ratio 0.8!
----------------------------------------------------
----------------------------------------------------




----------------------------------------------------
----------------------------------------------------
Pruning started for sparsity ratio 0.9!
----------------------------------------------------
----------------------------------------------------
{'train_runtime': 19645.7706, 'train_samples_per_second': 0.013, 'train_steps_per_second': 0.013, 'train_loss': 4.548011302947998, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =      4.548
  train_runtime            = 5:27:25.77
  train_samples_per_second =      0.013
  train_steps_per_second   =      0.013
----------------------------------------------------
----------------------------------------------------
Pruning finished for sparsity ratio 0.9!
----------------------------------------------------
----------------------------------------------------
G] Force convert framework model to neural_compressor model.
2023-05-21 17:16:07 [INFO] Start to get the baseline model's score before pruning.
2023-05-21 17:16:07 [INFO] Baseline model's score is [].
2023-05-21 17:16:07 [INFO] Model pruning begins.
2023-05-21 17:16:07 [INFO] The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: output, input, code.
/home/dsochirca/.conda/envs/test/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
2023-05-21 17:16:07 [INFO] ***** Running training *****
2023-05-21 17:16:07 [INFO]   Num examples = 128
2023-05-21 17:16:07 [INFO]   Num Epochs = 2
2023-05-21 17:16:07 [INFO]   Instantaneous batch size per device = 1
2023-05-21 17:16:07 [INFO]   Total train batch size (w. parallel, distributed & accumulation) = 1
2023-05-21 17:16:07 [INFO]   Gradient Accumulation steps = 1
2023-05-21 17:16:07 [INFO]   Total optimization steps = 256
  0%|          | 0/256 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/dsochirca/.conda/envs/test/lib/python3.7/site-packages/neural_compressor/experimental/pruner_legacy/group_lasso.py:51: RuntimeWarning: divide by zero encountered in true_divide
  coeff = self.alpha / np.linalg.norm(reshaped_weight, 2, axis=(1,3))
  0%|          | 1/256 [03:15<13:49:41, 195.22s/it]  1%|          | 2/256 [04:26<8:37:22, 122.22s/it]   1%|          | 3/256 [05:35<6:53:42, 98.11s/it]   2%|▏         | 4/256 [06:44<6:04:05, 86.69s/it]  2%|▏         | 5/256 [07:55<5:37:39, 80.72s/it]  2%|▏         | 6/256 [09:04<5:19:45, 76.74s/it]  3%|▎         | 7/256 [10:13<5:08:54, 74.44s/it]  3%|▎         | 8/256 [11:23<5:00:48, 72.78s/it]  4%|▎         | 9/256 [12:33<4:56:31, 72.03s/it]  4%|▍         | 10/256 [13:42<4:51:56, 71.21s/it]  4%|▍         | 11/256 [14:51<4:48:03, 70.54s/it]  5%|▍         | 12/256 [16:01<4:46:13, 70.38s/it]  5%|▌         | 13/256 [17:11<4:43:36, 70.03s/it]  5%|▌         | 14/256 [18:21<4:42:31, 70.05s/it]  6%|▌         | 15/256 [19:30<4:40:16, 69.78s/it]  6%|▋         | 16/256 [20:39<4:38:59, 69.75s/it]  7%|▋         | 17/256 [21:48<4:36:40, 69.46s/it]  7%|▋         | 18/256 [22:58<4:36:04, 69.60s/it]  7%|▋         | 19/256 [24:08<4:35:16, 69.69s/it]  8%|▊         | 20/256 [25:18<4:34:09, 69.70s/it]  8%|▊         | 21/256 [26:26<4:31:46, 69.39s/it]  9%|▊         | 22/256 [27:35<4:29:43, 69.16s/it]  9%|▉         | 23/256 [28:48<4:32:26, 70.16s/it]  9%|▉         | 24/256 [29:57<4:29:54, 69.80s/it] 10%|▉         | 25/256 [31:06<4:28:14, 69.67s/it] 10%|█         | 26/256 [32:16<4:27:06, 69.68s/it] 11%|█         | 27/256 [33:25<4:25:22, 69.53s/it] 11%|█         | 28/256 [34:34<4:23:52, 69.44s/it] 11%|█▏        | 29/256 [35:44<4:23:36, 69.67s/it] 12%|█▏        | 30/256 [36:53<4:21:51, 69.52s/it] 12%|█▏        | 31/256 [38:03<4:20:30, 69.47s/it] 12%|█▎        | 32/256 [39:13<4:20:22, 69.75s/it] 13%|█▎        | 33/256 [40:22<4:18:38, 69.59s/it] 13%|█▎        | 34/256 [41:31<4:16:26, 69.31s/it] 14%|█▎        | 35/256 [42:40<4:15:03, 69.25s/it] 14%|█▍        | 36/256 [43:50<4:14:34, 69.43s/it] 14%|█▍        | 37/256 [45:01<4:15:22, 69.97s/it] 15%|█▍        | 38/256 [46:10<4:13:00, 69.63s/it] 15%|█▌        | 39/256 [47:17<4:09:22, 68.95s/it] 16%|█▌        | 40/256 [48:25<4:06:19, 68.43s/it] 16%|█▌        | 41/256 [49:33<4:05:32, 68.53s/it] 16%|█▋        | 42/256 [50:42<4:04:10, 68.46s/it] 17%|█▋        | 43/256 [51:48<4:00:54, 67.86s/it] 17%|█▋        | 44/256 [52:56<3:59:38, 67.83s/it] 18%|█▊        | 45/256 [54:03<3:57:39, 67.58s/it] 18%|█▊        | 46/256 [55:12<3:58:25, 68.12s/it] 18%|█▊        | 47/256 [56:21<3:58:02, 68.34s/it] 19%|█▉        | 48/256 [57:31<3:58:52, 68.91s/it] 19%|█▉        | 49/256 [58:46<4:03:09, 70.48s/it] 20%|█▉        | 50/256 [59:56<4:02:04, 70.51s/it] 20%|█▉        | 51/256 [1:01:15<4:09:02, 72.89s/it] 20%|██        | 52/256 [1:02:30<4:10:53, 73.79s/it] 21%|██        | 53/256 [1:03:41<4:05:55, 72.69s/it] 21%|██        | 54/256 [1:04:57<4:08:32, 73.83s/it] 21%|██▏       | 55/256 [1:06:13<4:09:31, 74.49s/it] 22%|██▏       | 56/256 [1:07:32<4:12:28, 75.74s/it] 22%|██▏       | 57/256 [1:08:48<4:11:41, 75.89s/it] 23%|██▎       | 58/256 [1:10:03<4:09:49, 75.71s/it] 23%|██▎       | 59/256 [1:11:19<4:08:29, 75.68s/it] 23%|██▎       | 60/256 [1:12:35<4:08:10, 75.97s/it] 24%|██▍       | 61/256 [1:13:51<4:06:48, 75.94s/it] 24%|██▍       | 62/256 [1:15:07<4:05:24, 75.90s/it] 25%|██▍       | 63/256 [1:16:25<4:06:15, 76.56s/it] 25%|██▌       | 64/256 [1:17:41<4:04:40, 76.46s/it] 25%|██▌       | 65/256 [1:18:57<4:02:06, 76.06s/it] 26%|██▌       | 66/256 [1:20:12<4:00:11, 75.85s/it] 26%|██▌       | 67/256 [1:21:29<4:00:03, 76.21s/it] 27%|██▋       | 68/256 [1:22:42<3:55:19, 75.10s/it] 27%|██▋       | 69/256 [1:23:58<3:55:08, 75.44s/it] 27%|██▋       | 70/256 [1:25:09<3:50:00, 74.20s/it] 28%|██▊       | 71/256 [1:26:28<3:52:42, 75.47s/it] 28%|██▊       | 72/256 [1:27:43<3:51:43, 75.56s/it] 29%|██▊       | 73/256 [1:28:52<3:44:31, 73.62s/it] 29%|██▉       | 74/256 [1:30:07<3:44:05, 73.88s/it] 29%|██▉       | 75/256 [1:31:19<3:41:06, 73.30s/it] 30%|██▉       | 76/256 [1:32:36<3:43:12, 74.40s/it] 30%|███       | 77/256 [1:33:53<3:44:32, 75.27s/it] 30%|███       | 78/256 [1:35:12<3:46:25, 76.33s/it] 31%|███       | 79/256 [1:36:31<3:47:20, 77.06s/it] 31%|███▏      | 80/256 [1:37:48<3:46:42, 77.29s/it] 32%|███▏      | 81/256 [1:39:04<3:43:59, 76.79s/it] 32%|███▏      | 82/256 [1:40:21<3:43:03, 76.92s/it] 32%|███▏      | 83/256 [1:41:33<3:37:09, 75.31s/it] 33%|███▎      | 84/256 [1:42:50<3:37:04, 75.72s/it] 33%|███▎      | 85/256 [1:44:03<3:33:50, 75.03s/it] 34%|███▎      | 86/256 [1:45:18<3:32:46, 75.10s/it] 34%|███▍      | 87/256 [1:46:24<3:23:23, 72.21s/it] 34%|███▍      | 88/256 [1:47:29<3:16:42, 70.25s/it] 35%|███▍      | 89/256 [1:48:35<3:11:40, 68.87s/it] 35%|███▌      | 90/256 [1:49:42<3:09:06, 68.35s/it] 36%|███▌      | 91/256 [1:51:00<3:15:43, 71.18s/it] 36%|███▌      | 92/256 [1:52:17<3:19:02, 72.82s/it] 36%|███▋      | 93/256 [1:53:30<3:18:19, 73.00s/it] 37%|███▋      | 94/256 [1:54:46<3:19:29, 73.89s/it] 37%|███▋      | 95/256 [1:56:01<3:18:55, 74.13s/it] 38%|███▊      | 96/256 [1:57:19<3:21:22, 75.52s/it] 38%|███▊      | 97/256 [1:58:35<3:20:11, 75.55s/it] 38%|███▊      | 98/256 [1:59:52<3:20:13, 76.03s/it] 39%|███▊      | 99/256 [2:01:10<3:20:24, 76.59s/it] 39%|███▉      | 100/256 [2:02:26<3:18:33, 76.37s/it] 39%|███▉      | 101/256 [2:03:42<3:17:12, 76.34s/it] 40%|███▉      | 102/256 [2:04:55<3:12:56, 75.17s/it] 40%|████      | 103/256 [2:06:10<3:12:04, 75.32s/it] 41%|████      | 104/256 [2:07:26<3:11:02, 75.41s/it] 41%|████      | 105/256 [2:08:44<3:11:31, 76.10s/it] 41%|████▏     | 106/256 [2:10:02<3:11:46, 76.71s/it] 42%|████▏     | 107/256 [2:11:15<3:07:51, 75.65s/it] 42%|████▏     | 108/256 [2:12:22<2:59:57, 72.96s/it] 43%|████▎     | 109/256 [2:13:27<2:52:47, 70.53s/it] 43%|████▎     | 110/256 [2:14:32<2:48:16, 69.16s/it] 43%|████▎     | 111/256 [2:15:50<2:53:26, 71.77s/it] 44%|████▍     | 112/256 [2:17:09<2:56:54, 73.71s/it] 44%|████▍     | 113/256 [2:18:22<2:55:45, 73.75s/it] 45%|████▍     | 114/256 [2:19:32<2:51:49, 72.60s/it] 45%|████▍     | 115/256 [2:20:42<2:48:47, 71.82s/it] 45%|████▌     | 116/256 [2:21:59<2:51:05, 73.32s/it] 46%|████▌     | 117/256 [2:23:10<2:47:49, 72.44s/it] 46%|████▌     | 118/256 [2:24:26<2:49:22, 73.64s/it] 46%|████▋     | 119/256 [2:25:43<2:50:24, 74.63s/it] 47%|████▋     | 120/256 [2:26:52<2:45:17, 72.93s/it] 47%|████▋     | 121/256 [2:28:11<2:48:33, 74.91s/it] 48%|████▊     | 122/256 [2:29:29<2:48:56, 75.65s/it] 48%|████▊     | 123/256 [2:30:45<2:47:58, 75.78s/it] 48%|████▊     | 124/256 [2:32:04<2:48:40, 76.67s/it] 49%|████▉     | 125/256 [2:33:19<2:46:49, 76.41s/it] 49%|████▉     | 126/256 [2:34:31<2:42:41, 75.09s/it] 50%|████▉     | 127/256 [2:35:51<2:44:15, 76.40s/it] 50%|█████     | 128/256 [2:37:06<2:42:07, 76.00s/it]2023-05-21 19:53:15 [INFO]                                    Name  ... Sparsity(%)
0                transformer.wte.weight  ...       40.00
1                transformer.wpe.weight  ...       40.00
2             transformer.h.0.attn.bias  ...       49.95
3    transformer.h.0.attn.c_attn.weight  ...       40.00
4    transformer.h.0.attn.c_proj.weight  ...       40.00
..                                  ...  ...         ...
59  transformer.h.11.attn.c_proj.weight  ...       40.00
60     transformer.h.11.mlp.c_fc.weight  ...       40.00
61   transformer.h.11.mlp.c_proj.weight  ...       40.00
62                       lm_head.weight  ...       40.00
63                      Total sparsity:  ...       40.72

[64 rows x 5 columns]
2023-05-21 19:53:15 [INFO] 40.7150988076128
/home/dsochirca/.conda/envs/test/lib/python3.7/site-packages/neural_compressor/experimental/pruner_legacy/group_lasso.py:51: RuntimeWarning: divide by zero encountered in true_divide
  coeff = self.alpha / np.linalg.norm(reshaped_weight, 2, axis=(1,3))
 50%|█████     | 129/256 [2:39:21<3:18:22, 93.72s/it] 51%|█████     | 130/256 [2:40:37<3:05:33, 88.36s/it] 51%|█████     | 131/256 [2:41:52<2:55:49, 84.39s/it] 52%|█████▏    | 132/256 [2:43:08<2:49:15, 81.90s/it] 52%|█████▏    | 133/256 [2:44:23<2:43:26, 79.73s/it] 52%|█████▏    | 134/256 [2:45:39<2:40:05, 78.73s/it] 53%|█████▎    | 135/256 [2:46:53<2:35:38, 77.18s/it] 53%|█████▎    | 136/256 [2:48:09<2:33:46, 76.89s/it] 54%|█████▎    | 137/256 [2:49:21<2:29:29, 75.38s/it] 54%|█████▍    | 138/256 [2:50:32<2:26:03, 74.26s/it] 54%|█████▍    | 139/256 [2:51:49<2:26:11, 74.97s/it] 55%|█████▍    | 140/256 [2:53:04<2:24:55, 74.96s/it] 55%|█████▌    | 141/256 [2:54:19<2:23:40, 74.96s/it] 55%|█████▌    | 142/256 [2:55:33<2:22:06, 74.80s/it] 56%|█████▌    | 143/256 [2:56:44<2:18:38, 73.61s/it] 56%|█████▋    | 144/256 [2:57:57<2:17:09, 73.47s/it] 57%|█████▋    | 145/256 [2:59:14<2:17:41, 74.43s/it] 57%|█████▋    | 146/256 [3:00:31<2:17:49, 75.18s/it] 57%|█████▋    | 147/256 [3:01:44<2:15:30, 74.59s/it] 58%|█████▊    | 148/256 [3:02:58<2:13:54, 74.39s/it] 58%|█████▊    | 149/256 [3:04:13<2:12:56, 74.55s/it] 59%|█████▊    | 150/256 [3:05:25<2:10:27, 73.84s/it] 59%|█████▉    | 151/256 [3:06:38<2:08:53, 73.65s/it] 59%|█████▉    | 152/256 [3:07:47<2:05:03, 72.15s/it] 60%|█████▉    | 153/256 [3:08:55<2:01:37, 70.85s/it] 60%|██████    | 154/256 [3:10:01<1:58:05, 69.46s/it] 61%|██████    | 155/256 [3:11:11<1:56:55, 69.46s/it] 61%|██████    | 156/256 [3:12:25<1:58:29, 71.10s/it] 61%|██████▏   | 157/256 [3:13:42<2:00:04, 72.77s/it] 62%|██████▏   | 158/256 [3:14:59<2:00:39, 73.87s/it] 62%|██████▏   | 159/256 [3:16:15<2:00:39, 74.64s/it] 62%|██████▎   | 160/256 [3:17:29<1:59:19, 74.58s/it] 63%|██████▎   | 161/256 [3:18:45<1:58:22, 74.76s/it] 63%|██████▎   | 162/256 [3:20:01<1:57:52, 75.23s/it] 64%|██████▎   | 163/256 [3:21:09<1:53:22, 73.14s/it] 64%|██████▍   | 164/256 [3:22:19<1:50:22, 71.98s/it] 64%|██████▍   | 165/256 [3:23:36<1:51:48, 73.72s/it] 65%|██████▍   | 166/256 [3:24:53<1:51:44, 74.49s/it] 65%|██████▌   | 167/256 [3:26:01<1:47:55, 72.76s/it] 66%|██████▌   | 168/256 [3:27:16<1:47:25, 73.24s/it] 66%|██████▌   | 169/256 [3:28:33<1:47:51, 74.38s/it] 66%|██████▋   | 170/256 [3:29:48<1:47:02, 74.68s/it] 67%|██████▋   | 171/256 [3:30:57<1:43:25, 73.01s/it] 67%|██████▋   | 172/256 [3:32:07<1:40:51, 72.04s/it] 68%|██████▊   | 173/256 [3:33:22<1:40:43, 72.81s/it] 68%|██████▊   | 174/256 [3:34:37<1:40:35, 73.60s/it] 68%|██████▊   | 175/256 [3:35:49<1:38:34, 73.02s/it] 69%|██████▉   | 176/256 [3:37:00<1:36:44, 72.56s/it] 69%|██████▉   | 177/256 [3:38:08<1:33:39, 71.14s/it] 70%|██████▉   | 178/256 [3:39:28<1:35:49, 73.71s/it] 70%|██████▉   | 179/256 [3:40:48<1:37:13, 75.76s/it] 70%|███████   | 180/256 [3:42:07<1:37:12, 76.74s/it] 71%|███████   | 181/256 [3:43:27<1:37:03, 77.65s/it] 71%|███████   | 182/256 [3:44:48<1:36:58, 78.63s/it] 71%|███████▏  | 183/256 [3:46:07<1:35:44, 78.69s/it] 72%|███████▏  | 184/256 [3:47:26<1:34:26, 78.70s/it] 72%|███████▏  | 185/256 [3:48:44<1:33:03, 78.63s/it] 73%|███████▎  | 186/256 [3:50:04<1:32:10, 79.01s/it] 73%|███████▎  | 187/256 [3:51:25<1:31:29, 79.56s/it] 73%|███████▎  | 188/256 [3:52:46<1:30:42, 80.04s/it] 74%|███████▍  | 189/256 [3:54:00<1:27:28, 78.34s/it] 74%|███████▍  | 190/256 [3:55:18<1:25:50, 78.04s/it] 75%|███████▍  | 191/256 [3:56:37<1:24:49, 78.30s/it] 75%|███████▌  | 192/256 [3:57:57<1:24:17, 79.03s/it] 75%|███████▌  | 193/256 [3:59:18<1:23:30, 79.53s/it] 76%|███████▌  | 194/256 [4:00:38<1:22:20, 79.69s/it] 76%|███████▌  | 195/256 [4:01:59<1:21:32, 80.21s/it] 77%|███████▋  | 196/256 [4:03:21<1:20:30, 80.51s/it] 77%|███████▋  | 197/256 [4:04:41<1:19:15, 80.60s/it] 77%|███████▋  | 198/256 [4:06:02<1:17:59, 80.68s/it] 78%|███████▊  | 199/256 [4:07:23<1:16:41, 80.72s/it] 78%|███████▊  | 200/256 [4:08:39<1:14:01, 79.32s/it] 79%|███████▊  | 201/256 [4:10:00<1:13:04, 79.72s/it] 79%|███████▉  | 202/256 [4:11:20<1:11:51, 79.84s/it] 79%|███████▉  | 203/256 [4:12:41<1:10:42, 80.04s/it] 80%|███████▉  | 204/256 [4:14:01<1:09:28, 80.17s/it] 80%|████████  | 205/256 [4:15:22<1:08:24, 80.47s/it] 80%|████████  | 206/256 [4:16:42<1:06:59, 80.38s/it] 81%|████████  | 207/256 [4:18:02<1:05:32, 80.26s/it] 81%|████████▏ | 208/256 [4:19:23<1:04:23, 80.49s/it] 82%|████████▏ | 209/256 [4:20:42<1:02:37, 79.94s/it] 82%|████████▏ | 210/256 [4:22:01<1:01:02, 79.63s/it] 82%|████████▏ | 211/256 [4:23:21<59:52, 79.83s/it]   83%|████████▎ | 212/256 [4:24:42<58:40, 80.01s/it] 83%|████████▎ | 213/256 [4:26:00<56:58, 79.51s/it] 84%|████████▎ | 214/256 [4:27:20<55:51, 79.80s/it] 84%|████████▍ | 215/256 [4:28:41<54:35, 79.90s/it] 84%|████████▍ | 216/256 [4:30:01<53:24, 80.12s/it] 85%|████████▍ | 217/256 [4:31:20<51:50, 79.76s/it] 85%|████████▌ | 218/256 [4:32:41<50:40, 80.01s/it] 86%|████████▌ | 219/256 [4:33:57<48:37, 78.85s/it] 86%|████████▌ | 220/256 [4:35:14<47:05, 78.48s/it] 86%|████████▋ | 221/256 [4:36:34<45:53, 78.67s/it] 87%|████████▋ | 222/256 [4:37:55<44:58, 79.36s/it] 87%|████████▋ | 223/256 [4:39:16<43:54, 79.84s/it] 88%|████████▊ | 224/256 [4:40:36<42:43, 80.12s/it] 88%|████████▊ | 225/256 [4:41:57<41:28, 80.28s/it] 88%|████████▊ | 226/256 [4:43:15<39:47, 79.57s/it] 89%|████████▊ | 227/256 [4:44:35<38:32, 79.75s/it] 89%|████████▉ | 228/256 [4:45:56<37:22, 80.09s/it] 89%|████████▉ | 229/256 [4:47:15<35:55, 79.83s/it] 90%|████████▉ | 230/256 [4:48:36<34:40, 80.03s/it] 90%|█████████ | 231/256 [4:49:54<33:06, 79.47s/it] 91%|█████████ | 232/256 [4:51:14<31:53, 79.71s/it] 91%|█████████ | 233/256 [4:52:33<30:27, 79.47s/it] 91%|█████████▏| 234/256 [4:53:54<29:15, 79.80s/it] 92%|█████████▏| 235/256 [4:55:10<27:31, 78.64s/it] 92%|█████████▏| 236/256 [4:56:26<25:59, 78.00s/it] 93%|█████████▎| 237/256 [4:57:43<24:37, 77.75s/it] 93%|█████████▎| 238/256 [4:59:01<23:21, 77.88s/it] 93%|█████████▎| 239/256 [5:00:19<22:01, 77.72s/it] 94%|█████████▍| 240/256 [5:01:39<20:56, 78.56s/it] 94%|█████████▍| 241/256 [5:02:57<19:34, 78.33s/it] 95%|█████████▍| 242/256 [5:04:15<18:15, 78.26s/it] 95%|█████████▍| 243/256 [5:05:35<17:04, 78.84s/it] 95%|█████████▌| 244/256 [5:06:56<15:52, 79.37s/it] 96%|█████████▌| 245/256 [5:08:15<14:32, 79.29s/it] 96%|█████████▌| 246/256 [5:09:35<13:14, 79.46s/it] 96%|█████████▋| 247/256 [5:10:56<11:58, 79.84s/it] 97%|█████████▋| 248/256 [5:12:15<10:38, 79.83s/it] 97%|█████████▋| 249/256 [5:13:33<09:14, 79.19s/it] 98%|█████████▊| 250/256 [5:14:54<07:57, 79.63s/it] 98%|█████████▊| 251/256 [5:16:15<06:40, 80.07s/it] 98%|█████████▊| 252/256 [5:17:36<05:21, 80.35s/it] 99%|█████████▉| 253/256 [5:18:51<03:56, 78.72s/it] 99%|█████████▉| 254/256 [5:20:11<02:38, 79.09s/it]100%|█████████▉| 255/256 [5:21:31<01:19, 79.38s/it]100%|██████████| 256/256 [5:22:51<00:00, 79.67s/it]2023-05-21 22:39:00 [INFO] Set transformer.wte.weight sparsity with mask 38400768 7680153 0.8000000156246875.
2023-05-21 22:39:00 [INFO] Set transformer.wpe.weight sparsity with mask 786432 157286 0.8000005086263021.
2023-05-21 22:39:00 [INFO] Set transformer.h.0.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.0.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.0.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.0.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.1.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.1.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.1.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.1.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.2.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.2.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.2.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.2.mlp.c_proj.weight sparsity with mask 2359296 471858 0.8000005086263021.
2023-05-21 22:39:00 [INFO] Set transformer.h.3.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.3.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.3.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.3.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.4.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.4.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.4.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.4.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.5.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.5.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.5.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.5.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.6.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.6.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.6.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.6.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.7.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.7.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.7.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.7.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.8.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.8.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.8.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.8.mlp.c_proj.weight sparsity with mask 2359296 471858 0.8000005086263021.
2023-05-21 22:39:00 [INFO] Set transformer.h.9.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.9.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.9.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.9.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.10.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:00 [INFO] Set transformer.h.10.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:00 [INFO] Set transformer.h.10.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.10.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:00 [INFO] Set transformer.h.11.attn.c_attn.weight sparsity with mask 1769472 353894 0.8000002260561343.
2023-05-21 22:39:01 [INFO] Set transformer.h.11.attn.c_proj.weight sparsity with mask 589824 117964 0.8000013563368056.
2023-05-21 22:39:01 [INFO] Set transformer.h.11.mlp.c_fc.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:01 [INFO] Set transformer.h.11.mlp.c_proj.weight sparsity with mask 2359296 471859 0.8000000847710503.
2023-05-21 22:39:01 [INFO]                                    Name  ... Sparsity(%)
0                transformer.wte.weight  ...       80.00
1                transformer.wpe.weight  ...       80.00
2             transformer.h.0.attn.bias  ...       49.95
3    transformer.h.0.attn.c_attn.weight  ...       80.00
4    transformer.h.0.attn.c_proj.weight  ...       80.00
..                                  ...  ...         ...
59  transformer.h.11.attn.c_proj.weight  ...       80.00
60     transformer.h.11.mlp.c_fc.weight  ...       80.00
61   transformer.h.11.mlp.c_proj.weight  ...       80.00
62                       lm_head.weight  ...       80.00
63                      Total sparsity:  ...       77.84

[64 rows x 5 columns]
2023-05-21 22:39:01 [INFO] 77.84073371615162
2023-05-21 22:39:01 [INFO] 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 256/256 [5:22:53<00:00, 79.67s/it]100%|██████████| 256/256 [5:22:53<00:00, 75.68s/it]
2023-05-21 22:39:01 [INFO] Saving model checkpoint to ./test_epoch
2023-05-21 22:39:06 [INFO] Model pruning is done. Start to evaluate the pruned model.
2023-05-21 22:39:06 [INFO] Pruned model score is [].
2023-05-21 22:39:06 [INFO]                                    Name  ... Sparsity(%)
0                transformer.wte.weight  ...       80.00
1                transformer.wpe.weight  ...       80.00
2             transformer.h.0.attn.bias  ...       49.95
3    transformer.h.0.attn.c_attn.weight  ...       80.00
4    transformer.h.0.attn.c_proj.weight  ...       80.00
..                                  ...  ...         ...
59  transformer.h.11.attn.c_proj.weight  ...       80.00
60     transformer.h.11.mlp.c_fc.weight  ...       80.00
61   transformer.h.11.mlp.c_proj.weight  ...       80.00
62                       lm_head.weight  ...       80.00
63                      Total sparsity:  ...       77.84

[64 rows x 5 columns]
2023-05-21 22:39:06 [INFO] 77.84073371615162
2023-05-21 22:39:06 [INFO] Saving model checkpoint to ./CodeGPT-Py150_pruned_80.0%_sparsity
2023-05-21 22:39:13 [WARNING] Force convert framework model to neural_compressor model.
2023-05-21 22:39:13 [INFO] Start to get the baseline model's score before pruning.
2023-05-21 22:39:13 [INFO] Baseline model's score is [].
2023-05-21 22:39:13 [INFO] Model pruning begins.
2023-05-21 22:39:13 [INFO] The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: output, input, code.
/home/dsochirca/.conda/envs/test/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
2023-05-21 22:39:13 [INFO] ***** Running training *****
2023-05-21 22:39:13 [INFO]   Num examples = 128
2023-05-21 22:39:13 [INFO]   Num Epochs = 2
2023-05-21 22:39:13 [INFO]   Instantaneous batch size per device = 1
2023-05-21 22:39:13 [INFO]   Total train batch size (w. parallel, distributed & accumulation) = 1
2023-05-21 22:39:13 [INFO]   Gradient Accumulation steps = 1
2023-05-21 22:39:13 [INFO]   Total optimization steps = 256
  0%|          | 0/256 [00:00<?, ?it/s]/home/dsochirca/.conda/envs/test/lib/python3.7/site-packages/neural_compressor/experimental/pruner_legacy/group_lasso.py:51: RuntimeWarning: divide by zero encountered in true_divide
  coeff = self.alpha / np.linalg.norm(reshaped_weight, 2, axis=(1,3))
  0%|          | 1/256 [01:52<7:59:38, 112.86s/it]  1%|          | 2/256 [03:11<6:33:34, 92.97s/it]   1%|          | 3/256 [04:31<6:07:01, 87.04s/it]  2%|▏         | 4/256 [05:52<5:55:03, 84.54s/it]  2%|▏         | 5/256 [07:12<5:47:12, 83.00s/it]  2%|▏         | 6/256 [08:28<5:35:54, 80.62s/it]  3%|▎         | 7/256 [09:48<5:32:34, 80.14s/it]  3%|▎         | 8/256 [11:09<5:32:27, 80.43s/it]  4%|▎         | 9/256 [12:29<5:31:38, 80.56s/it]  4%|▍         | 10/256 [13:50<5:30:40, 80.65s/it]  4%|▍         | 11/256 [15:10<5:28:31, 80.45s/it]  5%|▍         | 12/256 [16:27<5:22:43, 79.36s/it]  5%|▌         | 13/256 [17:48<5:23:52, 79.97s/it]  5%|▌         | 14/256 [19:08<5:21:59, 79.83s/it]  6%|▌         | 15/256 [20:29<5:21:40, 80.09s/it]  6%|▋         | 16/256 [21:49<5:20:51, 80.21s/it]  7%|▋         | 17/256 [23:05<5:13:44, 78.76s/it]  7%|▋         | 18/256 [24:20<5:08:41, 77.82s/it]  7%|▋         | 19/256 [25:42<5:11:49, 78.94s/it]  8%|▊         | 20/256 [26:58<5:07:34, 78.20s/it]  8%|▊         | 21/256 [28:18<5:07:56, 78.62s/it]  9%|▊         | 22/256 [29:38<5:08:46, 79.17s/it]  9%|▉         | 23/256 [30:58<5:08:11, 79.36s/it]  9%|▉         | 24/256 [32:20<5:09:16, 79.99s/it] 10%|▉         | 25/256 [33:37<5:05:05, 79.25s/it] 10%|█         | 26/256 [34:58<5:06:15, 79.89s/it] 11%|█         | 27/256 [36:20<5:06:27, 80.29s/it] 11%|█         | 28/256 [37:40<5:05:40, 80.44s/it] 11%|█▏        | 29/256 [39:02<5:05:08, 80.65s/it] 12%|█▏        | 30/256 [40:23<5:04:29, 80.84s/it] 12%|█▏        | 31/256 [41:44<5:03:45, 81.00s/it] 12%|█▎        | 32/256 [43:05<5:01:32, 80.77s/it] 13%|█▎        | 33/256 [44:21<4:55:05, 79.40s/it] 13%|█▎        | 34/256 [45:41<4:55:07, 79.76s/it] 14%|█▎        | 35/256 [46:57<4:48:58, 78.46s/it] 14%|█▍        | 36/256 [48:16<4:48:51, 78.78s/it] 14%|█▍        | 37/256 [49:37<4:49:21, 79.28s/it] 15%|█▍        | 38/256 [50:56<4:48:26, 79.39s/it] 15%|█▌        | 39/256 [52:17<4:48:01, 79.64s/it] 16%|█▌        | 40/256 [53:38<4:48:44, 80.21s/it] 16%|█▌        | 41/256 [54:59<4:47:49, 80.32s/it] 16%|█▋        | 42/256 [56:19<4:46:04, 80.21s/it] 17%|█▋        | 43/256 [57:36<4:41:24, 79.27s/it] 17%|█▋        | 44/256 [58:56<4:41:38, 79.71s/it] 18%|█▊        | 45/256 [1:00:17<4:40:57, 79.89s/it] 18%|█▊        | 46/256 [1:01:38<4:40:43, 80.21s/it] 18%|█▊        | 47/256 [1:02:59<4:40:02, 80.39s/it] 19%|█▉        | 48/256 [1:04:18<4:37:46, 80.13s/it] 19%|█▉        | 49/256 [1:05:39<4:36:55, 80.27s/it] 20%|█▉        | 50/256 [1:06:58<4:34:39, 80.00s/it] 20%|█▉        | 51/256 [1:08:18<4:33:47, 80.13s/it] 20%|██        | 52/256 [1:09:39<4:33:06, 80.33s/it] 21%|██        | 53/256 [1:11:00<4:32:20, 80.50s/it] 21%|██        | 54/256 [1:12:21<4:31:15, 80.57s/it] 21%|██▏       | 55/256 [1:13:39<4:27:21, 79.81s/it] 22%|██▏       | 56/256 [1:14:58<4:25:41, 79.71s/it] 22%|██▏       | 57/256 [1:16:18<4:24:28, 79.74s/it] 23%|██▎       | 58/256 [1:17:38<4:23:33, 79.86s/it] 23%|██▎       | 59/256 [1:18:57<4:21:18, 79.59s/it] 23%|██▎       | 60/256 [1:20:10<4:13:07, 77.49s/it] 24%|██▍       | 61/256 [1:21:30<4:14:06, 78.19s/it] 24%|██▍       | 62/256 [1:22:47<4:12:03, 77.96s/it] 25%|██▍       | 63/256 [1:23:58<4:04:08, 75.90s/it] 25%|██▌       | 64/256 [1:25:19<4:07:23, 77.31s/it] 25%|██▌       | 65/256 [1:26:36<4:06:27, 77.42s/it] 26%|██▌       | 66/256 [1:27:58<4:08:47, 78.56s/it] 26%|██▌       | 67/256 [1:29:17<4:08:14, 78.81s/it] 27%|██▋       | 68/256 [1:30:37<4:08:25, 79.28s/it] 27%|██▋       | 69/256 [1:31:58<4:08:02, 79.59s/it] 27%|██▋       | 70/256 [1:33:19<4:07:53, 79.96s/it] 28%|██▊       | 71/256 [1:34:37<4:04:54, 79.43s/it] 28%|██▊       | 72/256 [1:35:56<4:03:23, 79.37s/it] 29%|██▊       | 73/256 [1:37:17<4:03:26, 79.82s/it] 29%|██▉       | 74/256 [1:38:36<4:01:27, 79.60s/it] 29%|██▉       | 75/256 [1:39:54<3:58:16, 78.98s/it] 30%|██▉       | 76/256 [1:41:15<3:58:44, 79.58s/it] 30%|███       | 77/256 [1:42:35<3:58:31, 79.95s/it] 30%|███       | 78/256 [1:43:56<3:57:56, 80.20s/it] 31%|███       | 79/256 [1:45:17<3:57:18, 80.44s/it] 31%|███▏      | 80/256 [1:46:37<3:55:35, 80.31s/it] 32%|███▏      | 81/256 [1:47:58<3:54:17, 80.33s/it] 32%|███▏      | 82/256 [1:49:17<3:52:38, 80.22s/it] 32%|███▏      | 83/256 [1:50:37<3:51:03, 80.14s/it] 33%|███▎      | 84/256 [1:51:58<3:50:07, 80.28s/it] 33%|███▎      | 85/256 [1:53:18<3:48:23, 80.14s/it] 34%|███▎      | 86/256 [1:54:36<3:45:29, 79.58s/it] 34%|███▍      | 87/256 [1:55:54<3:42:23, 78.96s/it] 34%|███▍      | 88/256 [1:57:14<3:42:13, 79.37s/it] 35%|███▍      | 89/256 [1:58:33<3:41:03, 79.42s/it] 35%|███▌      | 90/256 [1:59:49<3:36:18, 78.19s/it] 36%|███▌      | 91/256 [2:01:10<3:37:18, 79.02s/it] 36%|███▌      | 92/256 [2:02:29<3:35:55, 78.99s/it] 36%|███▋      | 93/256 [2:03:47<3:34:18, 78.89s/it] 37%|███▋      | 94/256 [2:05:06<3:32:29, 78.70s/it] 37%|███▋      | 95/256 [2:06:26<3:32:40, 79.26s/it] 38%|███▊      | 96/256 [2:07:46<3:32:03, 79.52s/it] 38%|███▊      | 97/256 [2:09:07<3:31:33, 79.83s/it] 38%|███▊      | 98/256 [2:10:27<3:30:27, 79.92s/it] 39%|███▊      | 99/256 [2:11:48<3:29:43, 80.15s/it] 39%|███▉      | 100/256 [2:13:08<3:28:41, 80.27s/it] 39%|███▉      | 101/256 [2:14:29<3:27:32, 80.34s/it] 40%|███▉      | 102/256 [2:15:49<3:26:22, 80.40s/it] 40%|████      | 103/256 [2:17:10<3:25:07, 80.44s/it] 41%|████      | 104/256 [2:18:30<3:23:54, 80.49s/it] 41%|████      | 105/256 [2:19:50<3:21:52, 80.21s/it] 41%|████▏     | 106/256 [2:21:10<3:20:18, 80.12s/it] 42%|████▏     | 107/256 [2:22:30<3:19:12, 80.22s/it] 42%|████▏     | 108/256 [2:23:51<3:18:30, 80.47s/it] 43%|████▎     | 109/256 [2:25:12<3:17:27, 80.59s/it] 43%|████▎     | 110/256 [2:26:31<3:14:51, 80.08s/it] 43%|████▎     | 111/256 [2:27:43<3:07:44, 77.68s/it] 44%|████▍     | 112/256 [2:28:54<3:01:16, 75.53s/it] 44%|████▍     | 113/256 [2:30:07<2:58:15, 74.79s/it] 45%|████▍     | 114/256 [2:31:19<2:55:21, 74.09s/it] 45%|████▍     | 115/256 [2:32:26<2:49:05, 71.95s/it] 45%|████▌     | 116/256 [2:33:35<2:45:53, 71.10s/it] 46%|████▌     | 117/256 [2:34:50<2:47:32, 72.32s/it] 46%|████▌     | 118/256 [2:36:02<2:45:41, 72.04s/it] 46%|████▋     | 119/256 [2:37:14<2:44:49, 72.18s/it] 47%|████▋     | 120/256 [2:38:27<2:44:12, 72.44s/it] 47%|████▋     | 121/256 [2:39:41<2:43:48, 72.80s/it] 48%|████▊     | 122/256 [2:40:54<2:42:41, 72.85s/it] 48%|████▊     | 123/256 [2:42:08<2:42:25, 73.27s/it] 48%|████▊     | 124/256 [2:43:21<2:41:02, 73.20s/it] 49%|████▉     | 125/256 [2:44:34<2:39:12, 72.92s/it] 49%|████▉     | 126/256 [2:45:49<2:39:47, 73.75s/it] 50%|████▉     | 127/256 [2:47:03<2:38:13, 73.59s/it] 50%|█████     | 128/256 [2:48:15<2:36:33, 73.39s/it]2023-05-22 01:27:30 [INFO]                                    Name  ... Sparsity(%)
0                transformer.wte.weight  ...       80.00
1                transformer.wpe.weight  ...       80.00
2             transformer.h.0.attn.bias  ...       49.95
3    transformer.h.0.attn.c_attn.weight  ...       80.00
4    transformer.h.0.attn.c_proj.weight  ...       80.00
..                                  ...  ...         ...
59  transformer.h.11.attn.c_proj.weight  ...       80.00
60     transformer.h.11.mlp.c_fc.weight  ...       80.00
61   transformer.h.11.mlp.c_proj.weight  ...       80.00
62                       lm_head.weight  ...       80.00
63                      Total sparsity:  ...       77.84

[64 rows x 5 columns]
2023-05-22 01:27:30 [INFO] 77.84073371615162
/home/dsochirca/.conda/envs/test/lib/python3.7/site-packages/neural_compressor/experimental/pruner_legacy/group_lasso.py:51: RuntimeWarning: divide by zero encountered in true_divide
  coeff = self.alpha / np.linalg.norm(reshaped_weight, 2, axis=(1,3))
 50%|█████     | 129/256 [2:50:02<2:56:20, 83.31s/it] 51%|█████     | 130/256 [2:51:18<2:50:12, 81.05s/it] 51%|█████     | 131/256 [2:52:31<2:44:18, 78.87s/it] 52%|█████▏    | 132/256 [2:53:46<2:40:13, 77.52s/it] 52%|█████▏    | 133/256 [2:54:55<2:34:01, 75.14s/it] 52%|█████▏    | 134/256 [2:56:05<2:29:25, 73.49s/it] 53%|█████▎    | 135/256 [2:57:18<2:28:02, 73.41s/it] 53%|█████▎    | 136/256 [2:58:34<2:28:12, 74.10s/it] 54%|█████▎    | 137/256 [2:59:46<2:25:38, 73.44s/it] 54%|█████▍    | 138/256 [3:00:59<2:24:30, 73.48s/it] 54%|█████▍    | 139/256 [3:02:10<2:21:24, 72.52s/it] 55%|█████▍    | 140/256 [3:03:21<2:19:30, 72.16s/it] 55%|█████▌    | 141/256 [3:04:34<2:18:34, 72.30s/it] 55%|█████▌    | 142/256 [3:05:42<2:14:51, 70.98s/it] 56%|█████▌    | 143/256 [3:06:54<2:14:35, 71.46s/it] 56%|█████▋    | 144/256 [3:08:10<2:16:04, 72.90s/it] 57%|█████▋    | 145/256 [3:09:25<2:15:58, 73.50s/it] 57%|█████▋    | 146/256 [3:10:41<2:15:49, 74.09s/it] 57%|█████▋    | 147/256 [3:11:53<2:13:28, 73.47s/it] 58%|█████▊    | 148/256 [3:13:07<2:12:34, 73.65s/it] 58%|█████▊    | 149/256 [3:14:23<2:12:52, 74.51s/it] 59%|█████▊    | 150/256 [3:15:38<2:11:33, 74.47s/it] 59%|█████▉    | 151/256 [3:16:54<2:11:26, 75.11s/it] 59%|█████▉    | 152/256 [3:18:08<2:09:28, 74.70s/it] 60%|█████▉    | 153/256 [3:19:24<2:08:59, 75.14s/it] 60%|██████    | 154/256 [3:20:33<2:04:33, 73.27s/it] 61%|██████    | 155/256 [3:21:43<2:01:36, 72.24s/it] 61%|██████    | 156/256 [3:22:58<2:01:39, 73.00s/it] 61%|██████▏   | 157/256 [3:24:13<2:01:44, 73.78s/it] 62%|██████▏   | 158/256 [3:25:27<2:00:36, 73.85s/it] 62%|██████▏   | 159/256 [3:26:40<1:58:55, 73.56s/it] 62%|██████▎   | 160/256 [3:27:53<1:57:29, 73.43s/it] 63%|██████▎   | 161/256 [3:29:14<1:59:42, 75.61s/it] 63%|██████▎   | 162/256 [3:30:24<1:55:57, 74.01s/it] 64%|██████▎   | 163/256 [3:31:44<1:57:11, 75.61s/it] 64%|██████▍   | 164/256 [3:32:55<1:53:52, 74.27s/it] 64%|██████▍   | 165/256 [3:34:10<1:52:58, 74.49s/it] 65%|██████▍   | 166/256 [3:35:22<1:50:42, 73.81s/it] 65%|██████▌   | 167/256 [3:36:38<1:50:14, 74.32s/it] 66%|██████▌   | 168/256 [3:37:53<1:49:38, 74.75s/it] 66%|██████▌   | 169/256 [3:39:04<1:46:33, 73.49s/it] 66%|██████▋   | 170/256 [3:40:18<1:45:23, 73.53s/it] 67%|██████▋   | 171/256 [3:41:34<1:45:31, 74.49s/it] 67%|██████▋   | 172/256 [3:42:50<1:44:50, 74.89s/it] 68%|██████▊   | 173/256 [3:44:10<1:45:49, 76.50s/it] 68%|██████▊   | 174/256 [3:45:30<1:45:59, 77.55s/it] 68%|██████▊   | 175/256 [3:46:46<1:43:56, 76.99s/it] 69%|██████▉   | 176/256 [3:48:00<1:41:23, 76.04s/it] 69%|██████▉   | 177/256 [3:49:15<1:39:40, 75.70s/it] 70%|██████▉   | 178/256 [3:50:28<1:37:18, 74.85s/it] 70%|██████▉   | 179/256 [3:51:44<1:36:27, 75.16s/it] 70%|███████   | 180/256 [3:52:55<1:33:54, 74.13s/it] 71%|███████   | 181/256 [3:54:06<1:31:30, 73.21s/it] 71%|███████   | 182/256 [3:55:19<1:30:14, 73.17s/it] 71%|███████▏  | 183/256 [3:56:36<1:30:22, 74.28s/it] 72%|███████▏  | 184/256 [3:57:51<1:29:22, 74.48s/it] 72%|███████▏  | 185/256 [3:59:03<1:27:01, 73.54s/it] 73%|███████▎  | 186/256 [4:00:16<1:25:46, 73.53s/it] 73%|███████▎  | 187/256 [4:01:28<1:23:55, 72.98s/it] 73%|███████▎  | 188/256 [4:02:46<1:24:30, 74.57s/it] 74%|███████▍  | 189/256 [4:04:01<1:23:33, 74.83s/it] 74%|███████▍  | 190/256 [4:05:17<1:22:38, 75.13s/it] 75%|███████▍  | 191/256 [4:06:34<1:21:55, 75.62s/it] 75%|███████▌  | 192/256 [4:07:47<1:19:43, 74.74s/it] 75%|███████▌  | 193/256 [4:08:55<1:16:17, 72.66s/it] 76%|███████▌  | 194/256 [4:10:08<1:15:11, 72.77s/it] 76%|███████▌  | 195/256 [4:11:22<1:14:37, 73.40s/it] 77%|███████▋  | 196/256 [4:12:34<1:12:58, 72.98s/it] 77%|███████▋  | 197/256 [4:13:51<1:12:44, 73.98s/it] 77%|███████▋  | 198/256 [4:15:06<1:11:48, 74.28s/it] 78%|███████▊  | 199/256 [4:16:23<1:11:19, 75.08s/it] 78%|███████▊  | 200/256 [4:17:36<1:09:42, 74.69s/it] 79%|███████▊  | 201/256 [4:18:48<1:07:28, 73.61s/it] 79%|███████▉  | 202/256 [4:20:00<1:05:58, 73.31s/it] 79%|███████▉  | 203/256 [4:21:18<1:05:57, 74.68s/it] 80%|███████▉  | 204/256 [4:22:34<1:05:10, 75.20s/it] 80%|████████  | 205/256 [4:23:49<1:03:40, 74.91s/it] 80%|████████  | 206/256 [4:25:07<1:03:23, 76.06s/it] 81%|████████  | 207/256 [4:26:25<1:02:29, 76.52s/it] 81%|████████▏ | 208/256 [4:27:38<1:00:15, 75.32s/it] 82%|████████▏ | 209/256 [4:28:56<59:41, 76.21s/it]   82%|████████▏ | 210/256 [4:30:07<57:09, 74.56s/it] 82%|████████▏ | 211/256 [4:31:22<56:13, 74.96s/it] 83%|████████▎ | 212/256 [4:32:39<55:16, 75.38s/it] 83%|████████▎ | 213/256 [4:33:52<53:28, 74.61s/it] 84%|████████▎ | 214/256 [4:35:07<52:26, 74.92s/it] 84%|████████▍ | 215/256 [4:36:22<51:11, 74.92s/it] 84%|████████▍ | 216/256 [4:37:34<49:16, 73.90s/it] 85%|████████▍ | 217/256 [4:38:51<48:47, 75.06s/it] 85%|████████▌ | 218/256 [4:40:04<47:07, 74.40s/it] 86%|████████▌ | 219/256 [4:41:23<46:45, 75.83s/it] 86%|████████▌ | 220/256 [4:42:39<45:27, 75.75s/it] 86%|████████▋ | 221/256 [4:43:53<43:47, 75.08s/it] 87%|████████▋ | 222/256 [4:45:09<42:44, 75.42s/it] 87%|████████▋ | 223/256 [4:46:26<41:48, 76.01s/it] 88%|████████▊ | 224/256 [4:47:40<40:11, 75.37s/it] 88%|████████▊ | 225/256 [4:48:54<38:39, 74.81s/it] 88%|████████▊ | 226/256 [4:50:11<37:46, 75.56s/it] 89%|████████▊ | 227/256 [4:51:26<36:31, 75.57s/it] 89%|████████▉ | 228/256 [4:52:43<35:27, 75.99s/it] 89%|████████▉ | 229/256 [4:53:55<33:37, 74.70s/it] 90%|████████▉ | 230/256 [4:55:11<32:28, 74.94s/it] 90%|█████████ | 231/256 [4:56:25<31:07, 74.70s/it] 91%|█████████ | 232/256 [4:57:36<29:27, 73.64s/it] 91%|█████████ | 233/256 [4:58:48<28:06, 73.31s/it] 91%|█████████▏| 234/256 [5:00:03<27:01, 73.69s/it] 92%|█████████▏| 235/256 [5:01:19<25:59, 74.28s/it] 92%|█████████▏| 236/256 [5:02:34<24:53, 74.66s/it] 93%|█████████▎| 237/256 [5:03:51<23:51, 75.32s/it] 93%|█████████▎| 238/256 [5:05:06<22:31, 75.08s/it] 93%|█████████▎| 239/256 [5:06:21<21:20, 75.30s/it] 94%|█████████▍| 240/256 [5:07:36<20:02, 75.14s/it] 94%|█████████▍| 241/256 [5:08:52<18:48, 75.24s/it] 95%|█████████▍| 242/256 [5:10:07<17:34, 75.35s/it] 95%|█████████▍| 243/256 [5:11:18<16:01, 73.99s/it] 95%|█████████▌| 244/256 [5:12:32<14:46, 73.83s/it] 96%|█████████▌| 245/256 [5:13:51<13:50, 75.51s/it] 96%|█████████▌| 246/256 [5:15:01<12:17, 73.78s/it] 96%|█████████▋| 247/256 [5:16:19<11:16, 75.17s/it] 97%|█████████▋| 248/256 [5:17:33<09:58, 74.85s/it] 97%|█████████▋| 249/256 [5:18:47<08:41, 74.48s/it] 98%|█████████▊| 250/256 [5:20:00<07:24, 74.09s/it] 98%|█████████▊| 251/256 [5:21:12<06:07, 73.49s/it] 98%|█████████▊| 252/256 [5:22:30<04:59, 74.88s/it] 99%|█████████▉| 253/256 [5:23:45<03:44, 74.92s/it] 99%|█████████▉| 254/256 [5:24:57<02:27, 73.95s/it]100%|█████████▉| 255/256 [5:26:10<01:13, 73.60s/it]100%|██████████| 256/256 [5:27:23<00:00, 73.39s/it]2023-05-22 04:06:38 [INFO] Set transformer.wte.weight sparsity with mask 38400768 3840073 0.9000000989563542.
2023-05-22 04:06:38 [INFO] Set transformer.wpe.weight sparsity with mask 786432 78643 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.0.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.0.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.0.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.0.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.1.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.1.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.1.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.1.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.2.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.2.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.2.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.2.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.3.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.3.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.3.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.3.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.4.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.4.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.4.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.4.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.5.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.5.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.5.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.5.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.6.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.6.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.6.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.6.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.7.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.7.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.7.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.7.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.8.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.8.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.8.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.8.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.9.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.9.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.9.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.9.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.10.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.10.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.10.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.10.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.11.attn.c_attn.weight sparsity with mask 1769472 176947 0.9000001130280672.
2023-05-22 04:06:38 [INFO] Set transformer.h.11.attn.c_proj.weight sparsity with mask 589824 58982 0.9000006781684028.
2023-05-22 04:06:38 [INFO] Set transformer.h.11.mlp.c_fc.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:38 [INFO] Set transformer.h.11.mlp.c_proj.weight sparsity with mask 2359296 235929 0.900000254313151.
2023-05-22 04:06:39 [INFO]                                    Name  ... Sparsity(%)
0                transformer.wte.weight  ...       90.00
1                transformer.wpe.weight  ...       90.00
2             transformer.h.0.attn.bias  ...       49.95
3    transformer.h.0.attn.c_attn.weight  ...       90.00
4    transformer.h.0.attn.c_proj.weight  ...       90.00
..                                  ...  ...         ...
59  transformer.h.11.attn.c_proj.weight  ...       90.00
60     transformer.h.11.mlp.c_fc.weight  ...       90.00
61   transformer.h.11.mlp.c_proj.weight  ...       90.00
62                       lm_head.weight  ...       90.00
63                      Total sparsity:  ...       87.12

[64 rows x 5 columns]
2023-05-22 04:06:39 [INFO] 87.12214729750177
2023-05-22 04:06:39 [INFO] 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 256/256 [5:27:25<00:00, 73.39s/it]100%|██████████| 256/256 [5:27:25<00:00, 76.74s/it]
2023-05-22 04:06:39 [INFO] Saving model checkpoint to ./test_epoch
2023-05-22 04:06:42 [INFO] Model pruning is done. Start to evaluate the pruned model.
2023-05-22 04:06:42 [INFO] Pruned model score is [].
2023-05-22 04:06:43 [INFO]                                    Name  ... Sparsity(%)
0                transformer.wte.weight  ...       90.00
1                transformer.wpe.weight  ...       90.00
2             transformer.h.0.attn.bias  ...       49.95
3    transformer.h.0.attn.c_attn.weight  ...       90.00
4    transformer.h.0.attn.c_proj.weight  ...       90.00
..                                  ...  ...         ...
59  transformer.h.11.attn.c_proj.weight  ...       90.00
60     transformer.h.11.mlp.c_fc.weight  ...       90.00
61   transformer.h.11.mlp.c_proj.weight  ...       90.00
62                       lm_head.weight  ...       90.00
63                      Total sparsity:  ...       87.12

[64 rows x 5 columns]
2023-05-22 04:06:43 [INFO] 87.12214729750177
2023-05-22 04:06:43 [INFO] Saving model checkpoint to ./CodeGPT-Py150_pruned_90.0%_sparsity
